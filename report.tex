%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{natbib}


\usepackage[hidelinks]{hyperref}
\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Ukrainian Catholic University}\\[1cm] % Name of your university/college
\textsc{\Large Applied Sciences Faculty}\\[0.5cm] % Major heading such as course name
\textsc{\large Data Science Master Programme}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Kernel Principal Component Analysis and its Applications}\\[10pt]
{\Large \bfseries Project report}\\[0.4cm] % Title of your document
\HRule \\[1cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------


% If you don't want a supervisor, uncomment the two lines below and remove the section above
\Large \emph{Authors:}\\
Irynei \textsc{Baran}\\Hanna \textsc{Pylieva}\\[1cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[height=4cm]{img/UCU-Apps.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\begin{abstract}
\todo{revise Abstract after finishing}
Principal component analysis (PCA) is a
popular tool for linear dimensionality reduction
and feature extraction. Kernel PCA
is the nonlinear form of PCA, which better
exploits the complicated spatial structure of
high-dimensional features. In this project, we
first review the basic ideas of PCA and kernel
PCA. Then we show some experimental
results to compare the performance
of kernel PCA and standard PCA for classification
problems. We also provide an overview of PCA and kPCA applications.
\end{abstract}

\section{Introduction}
\todo{rewrite the introduction}
In this section, we briefly review the principal component
analysis method, it's applications and limitations.

\section{Principal Component Analysis}
Principal component analysis, or PCA, is a mathematical procedure which is widely used for dimensionality reduction and feature selection. Those applications are achieved by projecting the data orthogonally onto a linear space with lower dimension, known as the principal subspace or feature space, such that the variance of projected data is maximal \citep*{bishop}.

Consider a data set $X$ containing $N$ observations of $D$ features ($D<N$). In order to visualize data or diminish number of features for modeling we want to reduce dimensionality of feature space to $M<D$. Whereas we are interested in most influential features to minimize data losses, that is why our goal is to maximize variance of projected data. The directions on which the data is projected called principal components. They are orthogonal and form coordinate system of subspace $M$ (see on ~\autoref{fig:pr-components}).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{img/geometric-PCA-8-both-components-with-plane.png}
	\caption{\label{fig:pr-components}Principal components}
\end{figure}

\subsection{Finding principal components}
To begin with, consider the projection on $M$ when $dim(M)=1$. Let a unit vector $u_1 \in D$ be the direction of $M$. Then projection of an observation $x_n\in X$ onto $M$ is $u_1^Tx_n$ and the variance of projected data is
\begin{equation}\label{var_proj}
 \dfrac{1}{N}\sum_{n=1}^{N}{\{u_1^T x_n - u_1^T \bar{x}\}}=u_1^T S u_1
\end{equation}	 
\todo{do we need to provide formula of covariance matrix explicitly? I don't feel so..}
where $\bar{x} = \frac{1}{N}\sum_{n=1}^{N} x_i$ is the mean of sample set and $S$ is the covariance matrix of data set $X$.

Maximization of \eqref{var_proj} is kept in a unit circle as we chose $u_1$ s.t. $||u_1||=u_1^T u_1=1$. So we need to find maximum of the next Lagrange function:
\begin{equation}
L(X,\lambda_1) = u_1^T S u_1 + \lambda_1(1-u_1^T u_1)
\end{equation}

By setting the derivative with respect to $u_1$ equal to zero we find that in stationary point $u_1$ needs to be an eigenvector of S:
\begin{equation}
S u_1 = \lambda_1 u_1
\end{equation}
Now when we left-multiply by $u^T_1$ and make use of $u^T_1 u_1 = 1$ we find out that the variance is given by
\begin{equation}
u_1^T S u_1 = \lambda_1 
\end{equation}
and so the variance will be a maximum when we set $u_1$ equal to the eigenvector
having the largest eigenvalue $\lambda_1$. This eigenvector id called the first principal
component \citep*{bishop}.

Next principal components can be found following the same procedure and choosing each new direction such that it maximizes the projected variance amongst all possible directions orthogonal to those already considered.


\subsection{Limitations of standard PCA}\label{limitations}
Although PCA is very useful from practical prospective it has some limitations.
\begin{enumerate}
	\item Assumptions of linear dependency.
	PCA projects data orthogonally to reduce dimensionality. This works only if data has linear dependent variables:  $y = k x + \epsilon$. Otherwise the method doesn't identify the direction where  projected data has highest possible variance which can be seen on   ~\autoref{fig:pca-cons-demo-1} . 
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{img/pca-linear-separation-demo.png}
		\caption{\label{fig:pca-cons-demo-1}Demonstration of PCA's low performance for non-linearly dependent data}
	\end{figure}
	
	\item Spread maximization. 
	PCA searches for a subspace where projected data has the maximal spread. However, this is not always the best way to represent data in lower-dimension subspace. A common example is the task of separating and counting pancakes from an image (see ~\autoref{fig:pca-cons-demo-2}). Important information for the task (number of pancakes) is located along Z axis which has the lowest variance. So Z axis will be the last principal component identified by PCA which is not what required. 
	
	\begin{figure}
		\centering
		\includegraphics[width=.5\textwidth, scale=0.4]{img/pancakes-white.png}
		\caption{\label{fig:pca-cons-demo-2}Image from  \href{http://golancourses.net/2014/kevan/01/23/3d-parametric-pancakes/}{3D Parametric Pancakes}
		}
	\end{figure}
	
	\item Principal components are hard to interpret.
	PCA reveals implicit dependencies in data which is non-trivial to interpret in case of high-dimensional space reduction. The leverage for this issue is deep understanding of domain.
	
	\item Orthogonality assumptions.
	PCA comes up with orthogonal principal components, which do not overlap in the space. For some tasks such functionality will produce wrong results (see ~\autoref{fig:pca-cons-demo-3}).
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{img/non-orthogonal-components.png}
		\caption{\label{fig:pca-cons-demo-3}Demonstration of PCA's low performance for non-linearly dependent data}
	\end{figure}

	\item Outliers and missing data sensitivity.
	We introduced PCA here as eigenvalue decomposition of a data covariance matrix. The latter is highly sensitive to sample outliers and corrupted data (also referred as intra-sample outliers) \citep{shlens}. This raises two issues:
		\begin{enumerate}
			\item PCA is a linear combinations of all input variables whereas in case of corrupted or missing entries presence we need to exclude some incomplete variables.
			
			\item in some contexts, outliers can be difficult to identify. In this case we are unable to remove them in order to get better performance from PCA. Whereas there is a way to overcome the issue by introducing robustness to the algorithm. This will be considered beneath.
		\end{enumerate}
	
	
\end{enumerate}

\section{Modifications of PCA}
Despite the limitations mentioned, PCA is still a powerful method for data analysis, visualization and dimensionality reduction. That is why numerous modifications were developed to overcome its drawbacks and broaden scope of applications. We will provide succinct description of the most common extensions of PCA.

\subsection{Robust PCA}
Gross errors in observations are now ubiquitous in modern applications
such as image processing, web data analysis, and bioinformatics, where some measurements
may be arbitrarily corrupted (due to occlusions, malicious tampering, or sensor failures) or simply irrelevant to the low-dimensional structure we seek to identify with PCA.The best algorithm which deals with data corruption is Robust PCA (RPCA).

Suppose the data under study can naturally be decomposed into low-rank ($L_0$) and sparse ($S_0$) components\footnote{This is not a synthetic requirement and can be done in a number of applications, refer to \citep{cardes} for examples.}:
\begin{equation}
M = L_0 + S_0
\end{equation}

RPCA allows to recover a low-rank matrix $L_0$ from highly corrupted measurements in $M$. Classical PCA works good when the noise term $N_0$ in $M$ is small. In contrast to this $S_0$ can have arbitrarily large magnitude, and their support is assumed to be sparse but unknown \citep{cardes}.

\subsection{Multilinear PCA}
Suppose we need to reduce dimensionality not of a simple matrix $M \in R^2$ but of a n-way array, i.e. a cube or hyper-cube of numbers, also informally referred to as a "data tensor". Common examples of tensor are 2-D/3-D images and video sequences. To deal with tensors PCA is generalized to multilinear PCA (MPCA). MPCA performs feature extraction by determining a multilinear projection that captures most of the original tensorial input variation. The solution is iterative in nature and it proceeds by decomposing the original problem to a series of multiple projection subproblems \citep{lu}. 

MPCA is applied to 3-D object recognition tasks \citep{sahambi} in machine vision, medical image analysis, space-time analysis of video sequences for activity recognition \citep{green} in human-computer interaction, etc.  MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.

\subsection{Nonlinear generalizations}
In real-world problems we often need to work with non-linear dependencies between features in data. For such tasks Hastie and Stuetzle \citep{hastie} proposed bending the loading
vectors to produce curves that approximate the nonlinear relationship between
a set of two variables. Such curves are called principal curves, their multidimensional extensions produce principal surfaces or principal manifolds \citep{gorban}.

Another popular way to use PCA in case of nonlinearity is performing it in a reproducing kernel Hilbert space associated with a positive definite kernel. 
\todo{Sentence needs revision. Now kept for purpose of structuring}
This algorithm is called Kernel PCA and rest of the report will be devoted to it.

\begin{thebibliography}{9}

\bibitem[Bishop (2009)]{bishop}
 Bishop, Christopher M.\textit{Pattern Recognition and Machine Learning}. Springer, Cambridge, U.K., 2006.

\bibitem[Shlens (2014)]{shlens}
 Shlens, Jonathon. \textit{A Tutorial on Principal Component Analysis}.Google Research, Mountain View, USA, 2014.

\bibitem[Cardes (2009)]{cardes}
 Candes, Emmanuel J.; Li, Xiaodong ; Ma, Yi; Wright,John. \textit{Robust Principal Component Analysis?} J.ACM, 58(11):1-37, 2009 
 
\bibitem[Lu (2008)]{lu} Lu, Haiping; Plataniotis, Konstantinos N.; Venetsanopoulos,  
 Anastasios N. 
 \textit{MPCA: Multilinear Principal Component Analysis of Tensor Objects}.  IEEE Transactions on Neural Networks, 19(1):18-39, 2008
 
\bibitem[Sahambi (2003)]{sahambi} 
 Sahambi, Harkirat S.; Khorasani, Khakhayar. \textit{A neural-network appearance-based 3-D object recognition using independent component analysis} IEEE Transactions on Neural Networks, 14(1): 138-149, 2003. 
 
\bibitem[Green (2004)]{green} 
 Green, Richard D.; Guan, Ling. \textit{Quantifying and recognizing human movement patterns from monocular video images - Part II: Applications to biometrics}. IEEE Transactions on Circuits and Systems for Video Technology, 14(2): 191-198, 2004. 
 
\bibitem[Hastie (1989)]{hastie}
 Hastie, Trevor; Stuetzle, Werner. \textit{Principal Curves}. Journal of the American Statistical Association, 84(406): 502-516, 1989.

\bibitem[Gorban (2007)]{gorban}
 Gorban,  Alexander N.; Kegl, Balazs; Wunsch, Donald C.; Zinovyev, Andrei. \textit{Principal Manifolds for Data Visualisation and Dimension Reduction}. Springer, Berlin – Heidelberg – New York, 2007. 
\end{thebibliography}

\end{document}