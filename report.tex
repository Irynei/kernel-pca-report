%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% University Assignment Title Page 
% LaTeX Template
% Version 1.0 (27/12/12)
%
% This template has been downloaded from:
% http://www.LaTeXTemplates.com
%
% Original author:
% WikiBooks (http://en.wikibooks.org/wiki/LaTeX/Title_Creation)
%
% License:
% CC BY-NC-SA 3.0 (http://creativecommons.org/licenses/by-nc-sa/3.0/)
% 
% Instructions for using this template:
% This title page is capable of being compiled as is. This is not useful for 
% including it in another document. To do this, you have two options: 
%
% 1) Copy/paste everything between \begin{document} and \end{document} 
% starting at \begin{titlepage} and paste this into another LaTeX file where you 
% want your title page.
% OR
% 2) Remove everything outside the \begin{titlepage} and \end{titlepage} and 
% move this file to the same directory as the LaTeX file you wish to add it to. 
% Then add \input{./title_page_1.tex} to your LaTeX file where you want your
% title page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\title{Title page with logo}
%----------------------------------------------------------------------------------------
%	PACKAGES AND OTHER DOCUMENT CONFIGURATIONS
%----------------------------------------------------------------------------------------

\documentclass[12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage{natbib}


\usepackage[hidelinks]{hyperref}
\begin{document}

\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------

\textsc{\LARGE Ukrainian Catholic University}\\[1cm] % Name of your university/college
\textsc{\Large Applied Sciences Faculty}\\[0.5cm] % Major heading such as course name
\textsc{\large Data Science Master Programme}\\[0.5cm] % Minor heading such as course title

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[0.4cm]
{ \huge \bfseries Kernel Principal Component Analysis and its Applications}\\[10pt]
{\Large \bfseries Project report}\\[0.4cm] % Title of your document
\HRule \\[1cm]
 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------


% If you don't want a supervisor, uncomment the two lines below and remove the section above
\Large \emph{Authors:}\\
Irynei \textsc{Baran}\\Hanna \textsc{Pylieva}\\[1cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

%----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

\includegraphics[height=4cm]{img/UCU-Apps.png}\\[1cm] % Include a department/university logo - this will require the graphicx package
 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}


\begin{abstract}
\todo{revise Abstract after finishing}
Principal component analysis (PCA) is a
popular tool for linear dimensionality reduction
and feature extraction. Kernel PCA
is the nonlinear form of PCA, which better
exploits the complicated spatial structure of
high-dimensional features. In this project, we
first review the basic ideas of PCA and kernel
PCA. Then we show some experimental
results to compare the performance
of kernel PCA and standard PCA for classification
problems. We also provide an overview of PCA and kPCA applications.
\end{abstract}

\section{Introduction}
\todo{rewrite the introduction}
In this section, we briefly review the principal component
analysis method, it's applications and limitations.

\section{Principal Component Analysis}
Principal component analysis, or PCA, is a mathematical procedure which is widely used for dimensionality reduction and feature selection. Those applications are achieved by projecting the data orthogonally onto a linear space with lower dimension, known as the principal subspace or feature space, such that the variance of projected data is maximal \citep*{bishop}.

Consider a data set $X$ containing $N$ observations of $D$ features ($D<N$). In order to visualize data or diminish number of features for modeling we want to reduce dimensionality of feature space to $M<D$. Whereas we are interested in most influential features to minimize data losses, that is why our goal is to maximize variance of projected data. The directions on which the data is projected called principal components. They are orthogonal and form coordinate system of subspace $M$ (see on ~\autoref{fig:pr-components}).

\begin{figure}[h]
	\centering
	\includegraphics[scale=0.3]{img/geometric-PCA-8-both-components-with-plane.png}
	\caption{\label{fig:pr-components}Principal components}
\end{figure}

\subsection{Finding principal components}
To begin with, consider the projection on $M$ when $dim(M)=1$. Let a unit vector $u_1 \in D$ be the direction of $M$. Then projection of an observation $x_n\in X$ onto $M$ is $u_1^Tx_n$ and the variance of projected data is
\begin{equation}\label{var_proj}
 \dfrac{1}{N}\sum_{n=1}^{N}{\{u_1^T x_n - u_1^T \bar{x}\}}=u_1^T S u_1
\end{equation}	 
\todo{do we need to provide formula of covariance matrix explicitly? I don't feel so..}
where $\bar{x} = \frac{1}{N}\sum_{n=1}^{N} x_i$ is the mean of sample set and $S$ is the covariance matrix of data set $X$.

Maximization of \eqref{var_proj} is kept in a unit circle as we chose $u_1$ s.t. $||u_1||=u_1^T u_1=1$. So we need to find maximum of the next Lagrange function:
\begin{equation}
L(X,\lambda_1) = u_1^T S u_1 + \lambda_1(1-u_1^T u_1)
\end{equation}

By setting the derivative with respect to $u_1$ equal to zero we find that in stationary point $u_1$ needs to be an eigenvector of S:
\begin{equation}
S u_1 = \lambda_1 u_1
\end{equation}
Now when we left-multiply by $u^T_1$ and make use of $u^T_1 u_1 = 1$ we find out that the variance is given by
\begin{equation}
u_1^T S u_1 = \lambda_1 
\end{equation}
and so the variance will be a maximum when we set $u_1$ equal to the eigenvector
having the largest eigenvalue $\lambda_1$. This eigenvector id called the first principal
component \citep*{bishop}.

Next principal components can be found following the same procedure and choosing each new direction such that it maximizes the projected variance amongst all possible directions orthogonal to those already considered.


\subsection{Limitations of standard PCA}\label{limitations}
Although PCA is very useful from practical prospective it has some limitations.
\begin{enumerate}
	\item Assumptions of linear dependency.
	PCA projects data orthogonally to reduce dimensionality. This works only if data has linear dependent variables:  $y = k x + \epsilon$. Otherwise the method doesn't identify the direction where  projected data has highest possible variance which can be seen on   ~\autoref{fig:pca-cons-demo-1} . 
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{img/pca-linear-separation-demo.png}
		\caption{\label{fig:pca-cons-demo-1}Demonstration of PCA's low performance for non-linearly dependent data}
	\end{figure}
	
	\item Spread maximization. 
	PCA searches for a subspace where projected data has the maximal spread. However, this is not always the best way to represent data in lower-dimension subspace. A common example is the task of separating and counting pancakes from an image (see ~\autoref{fig:pca-cons-demo-2}). Important information for the task (number of pancakes) is located along Z axis which has the lowest variance. So Z axis will be the last principal component identified by PCA which is not what required. 
	
	\begin{figure}
		\centering
		\includegraphics[width=.5\textwidth, scale=0.4]{img/pancakes-white.png}
		\caption{\label{fig:pca-cons-demo-2}Image from  \href{http://golancourses.net/2014/kevan/01/23/3d-parametric-pancakes/}{3D Parametric Pancakes}
		}
	\end{figure}
	
	\item Principal components are hard to interpret.
	PCA reveals implicit dependencies in data which is non-trivial to interpret in case of high-dimensional space reduction. The leverage for this issue is deep understanding of domain.
	
	\item Orthogonality assumptions.
	PCA comes up with orthogonal principal components, which do not overlap in the space. For some tasks such functionality will produce wrong results (see ~\autoref{fig:pca-cons-demo-3}).
	
	\begin{figure}[h]
		\centering
		\includegraphics[scale=0.5]{img/non-orthogonal-components.png}
		\caption{\label{fig:pca-cons-demo-3}Demonstration of PCA's low performance for non-linearly dependent data}
	\end{figure}

	\item Outliers and missing data sensitivity.
	We introduced PCA here as eigenvalue decomposition of a data covariance matrix. The latter is highly sensitive to sample outliers and corrupted data (also referred as intra-sample outliers) \citep{shlens}. This raises two issues:
		\begin{enumerate}
			\item PCA is a linear combinations of all input variables whereas in case of corrupted or missing entries presence we need to exclude some incomplete variables.
			
			\item in some contexts, outliers can be difficult to identify. In this case we are unable to remove them in order to get better performance from PCA. Whereas there is a way to overcome the issue by introducing robustness to the algorithm. This will be considered beneath.
		\end{enumerate}
	
	
\end{enumerate}

\section{Modifications of PCA}
Despite the limitations mentioned, PCA is still a powerful method for data analysis, visualization and dimensionality reduction. That is why numerous modifications were developed to overcome its drawbacks and broaden scope of applications. We will provide succinct description of the most common extensions of PCA.

\subsection{Robust PCA}
Gross errors in observations are now ubiquitous in modern applications
such as image processing, web data analysis, and bioinformatics, where some measurements
may be arbitrarily corrupted (due to occlusions, malicious tampering, or sensor failures) or simply irrelevant to the low-dimensional structure we seek to identify with PCA.The best algorithm which deals with data corruption is Robust PCA (RPCA).

Suppose the data under study can naturally be decomposed into low-rank ($L_0$) and sparse ($S_0$) components\footnote{This is not a synthetic requirement and can be done in a number of applications, refer to \citep{cardes} for examples.}:
\begin{equation}
M = L_0 + S_0
\end{equation}

RPCA allows to recover a low-rank matrix $L_0$ from highly corrupted measurements in $M$. Classical PCA works good when the noise term $N_0$ in $M$ is small. In contrast to this $S_0$ can have arbitrarily large magnitude, and their support is assumed to be sparse but unknown \citep{cardes}.


\subsection{Multilinear PCA}
Suppose we need to reduce dimensionality not of a simple matrix $M \in R^2$ but of a n-way array, i.e. a cube or hyper-cube of numbers, also informally referred to as a "data tensor". Common examples of tensor are 2-D/3-D images and video sequences. To deal with tensors PCA is generalized to multilinear PCA (MPCA). MPCA performs feature extraction by determining a multilinear projection that captures most of the original tensorial input variation. The solution is iterative in nature and it proceeds by decomposing the original problem to a series of multiple projection subproblems \citep{lu}. 

MPCA is applied to 3-D object recognition tasks \citep{sahambi} in machine vision, medical image analysis, space-time analysis of video sequences for activity recognition \citep{green} in human-computer interaction, etc.  MPCA is further extended to uncorrelated MPCA, non-negative MPCA and robust MPCA.

\subsection{Nonlinear generalizations}
In real-world problems we often need to work with non-linear dependencies between features in data. For such tasks Hastie and Stuetzle \citep{hastie} proposed bending the loading
vectors to produce curves that approximate the nonlinear relationship between
a set of two variables. Such curves are called principal curves, their multidimensional extensions produce principal surfaces or principal manifolds \citep{gorban}.

Another popular way to use PCA in case of nonlinearity is performing it in a reproducing kernel Hilbert space associated with a positive definite kernel. 
\todo{Sentence needs revision. Now kept for purpose of structuring}
This algorithm is called Kernel PCA and rest of the report will be devoted to it.

\section{Kernel Principal Component Analysis}
Kernel principal component analysis, or kPCA is a nonlinear generalization of PCA using techniques of kernel methods also known as ``kernel trick''. The main idea is to map the original data nonlinearly into a feature space $F$ by 
\begin{equation}
\phi: R^N \rightarrow F 
\end{equation}
and then perform PCA, which implicitly defines a nonlinear
principal components in the original data space (see ~\autoref{fig:kpca_bishop}). Even if $F$ has arbitrarily large dimensionality, for certain choices of $\phi$, it is still possible to perform PCA in $F$. This is done by the use of kernel functions \citep*{original_paper}.

\begin{figure}[ht]
	\centering
	\includegraphics[width=0.7\textwidth]{img/kpca_example_from_bishop.png}
	\caption{\label{fig:kpca_bishop}Kernel PCA. Data set in original data space (left-hand plot) and in the feature space $F$ (right-hand plot). \boldmath$v_1$ is a first principal component obtained in the feature space. Green lines indicate projection of data onto \boldmath$v_1$ \citep{bishop}}
\end{figure}


\subsection{Finding kPCA}
Consider a data set $X$ containing $N$ observations of $D$ features $(D < N)$ and a nonlinear transformation $\phi(x)$ into an $M$-dimensional feature space $F$. Let's assume that projected data set is centered, so $\dfrac{1}{N}\sum_{n=1}^{N}{\phi(x_n)}=0$.
The $M \times M$ covariance matrix in feature space is given by
\begin{equation}\label{cov_matrix}
C =  \dfrac{1}{N}\sum_{n=1}^{N}{\phi(x_n)\phi(x_n)^T}
\end{equation}

\todo{not sure if this subsection title sounds ok}
\subsubsection{Deriving eigenvalue equation}
We need to solve the following eigenvalue problem
\begin{equation}\label{eigenvector_eq}
Cv_i = \lambda_i v_i
\end{equation}
$i=1,...,M$. Our goal is to solve this equation without working directly in the feature space. Substituting $C$ using (\ref{cov_matrix}) we get
\begin{equation}
\dfrac{1}{N}\sum_{n=1}^{N}{\phi(x_n)\{\phi(x_n)^Tv_i\}} = \lambda_i v_i
\end{equation}

Provided $\lambda_i > 0$, the vector $v_i$ is given by a linear combination of the $\phi(x_n)$ and there exist coefficients $\alpha_1,...,\alpha_N$ such that \todo{Don't know if alphas is clear here}
\begin{equation}\label{eigenvector_by_alphas}
v_i = \sum_{n=1}^{N}{\alpha_{in}\phi(x_n)^T}
\end{equation}

Substituting (\ref{cov_matrix}) and (\ref{eigenvector_by_alphas}) into (\ref{eigenvector_eq}) we obtain
\begin{equation}\label{big_equation}
\dfrac{1}{N}\sum_{n=1}^{N}{\phi(x_n)\phi(x_n)^T} \sum_{m=1}^{N}{\alpha_{im}\phi(x_m)^T} = \lambda_i \sum_{n=1}^{N}{\alpha_{in}\phi(x_n)^T}
\end{equation}

Key thing here is to express (\ref{big_equation}) in terms of kernel function $k(x_n, x_m) = \phi(x_n)^T\phi(x_m)$, which we do by multiplying both sides by $\phi(x_l)^T$.

\begin{equation}
\dfrac{1}{N}\sum_{n=1}^{N}{k(x_l, x_m)} \sum_{m=1}^{N}{\alpha_{im}k(x_n, x_m)} = \lambda_i \sum_{n=1}^{N}{\alpha_{in}k(x_l, x_m)}
\end{equation}
or in matrix notation
\begin{equation}\label{K_1}
K^2\alpha_i = \lambda_iNK\alpha_i
\end{equation}
If we remove a factor of K from both sides we obtain following eigenvalue problem
\begin{equation}\label{K_2}
K\alpha_i = \lambda_iN\alpha_i
\end{equation}

We can find $\alpha_i$ by solving this eigenvalue problem. Note that solutions of (\ref{K_1}) and (\ref{K_2}) differ only by eigenvectors that correspond to zero eigenvalues of K, hence removing $K$ from both sides of (\ref{K_1}) does not affect principal components.  

The eigenvectors need to be orthonormal \todo{please check if this  any sense} in feature space, thus $\alpha_i$ needs to be normalized. Using (\ref{eigenvector_by_alphas}) and (\ref{K_2}), we get
\begin{equation}
1 = v_i^Tv_i = \sum_{n=1}^{N}\sum_{m=1}^{N}{\alpha_{in}\alpha_{im}\phi(x_n)^T\phi(x_m)} = \alpha_i^TK\alpha_i = \alpha_i^T\lambda_iN\alpha_i = \lambda_iN\alpha_i^T\alpha_i
\end{equation}

\subsubsection{Centering kernel}
So far, we assumed that projected data set has zero mean. But in general it will not be the case. The standard way to centralize data set is to compute mean and then subtract it from every data point.
Here we wish to avoid working in feature space and express everything in terms of kernel function. Let's denote projected data set after centering as $\widetilde{\phi}(x_n)$.

\begin{equation}
\widetilde{\phi}(x_n) = \phi(x_n) - \frac{1}{N}\sum_{l=1}^{N} x_l
\end{equation}
and the corresponding elements of the Gram matrix
\begin{equation}
\begin{aligned}
K_{nm} = \widetilde{\phi}(x_n)^T\widetilde{\phi}(x_m) = 
\phi(x_n)^T\phi(x_m) -
\sum_{l=1}^{N}{\phi(x_n)^T\phi(x_l)} - \\ \sum_{l=1}^{N}{\phi(x_l)^T\phi(x_m)} +  \dfrac{1}{N^2}\sum_{j=1}^{N}\sum_{l=1}^{N}{\phi(x_j)^T\phi(x_l)} = \\
k(x_n, x_m) - \sum_{l=1}^{N}{k(x_n, x_l)} - \sum_{l=1}^{N}{k(x_l, x_m)} + \dfrac{1}{N^2}\sum_{j=1}^{N}\sum_{l=1}^{N}{k(x_j, x_l)}
\end{aligned}
\end{equation}
or in a matrix notation
\begin{equation}
\widetilde{K} = K - 1_NK - K1_N + 1_NK1_N
\end{equation}
where $\widetilde{K}$ is centered kernel matrix, $1_N$ is $N \times N$ matrix in which every element equals to $\dfrac{1}{N}$ \citep{bishop}.
So, we are able to evaluate $\widetilde{K}$ using only kernel function.
\subsubsection{Finding principal component projection}
After solving the eigenvalue problem $\widetilde{K}\alpha_i = \lambda_i\alpha_i$, we can find projection onto principal components in terms of the kernel function.
Using (\ref{eigenvector_by_alphas}), projection of a point $x$ onto eigenvector $i$ is given by
\begin{equation}\label{projection_kpca}
\phi(x)^Tv_i = \sum_{n=1}^{N}{\alpha_{in}\phi(x)^T\phi(x_n)} = \sum_{n=1}^{N}{\alpha_{in}k(x, x_n)}
\end{equation}

Note that neither (\ref{eigenvector_eq})  nor (\ref{projection_kpca}) requires the $\phi(x)$ in explicit form, they are only needed in dot products. Therefore, we are able to use kernel functions for computing these dot products without actually performing the map $\phi$ \citep{original_paper}. 

\todo{isn't that too detailed explanation?}
In the original $D$-dimentional data space there are $D$ orthogonal eigenvectors and therefore we can find at most D principal components. The dimensionality $M$ of the feature space can be really large and even infinite, and thus it seems possible to find more than $D$ principal components. However, number of nonzero eigenvalues cannot exceed the number of observations $N$, because
the covariance matrix in feature space has rank at most $N$. So, kernel PCA leads to eigenvector expansion of the $N \times N$ kernel matrix $K$ \citep{bishop}.

\subsection{Implementation of kPCA}
Main steps of kPCA algorithm:
\begin{enumerate}
	\item Pick up a kernel function;
	\item Construct $M \times M$ dot product kernel matrix $K$ of original data set;
	\item Compute centered kernel matrix $\widetilde{K} = K - 1_NK - K1_N + 1_NK1_N$;
	\item Solve an eigenvalue problem $\widetilde{K}\alpha_i = \lambda_i\alpha_i$;
	\item Compute projections of any point $x$ onto eigenvectors (principal components);
\end{enumerate}
We are so cute that we managed to implement kPCA by ourselves. \href{https://github.com/Irynei/KernelPCA}{Here} you can find source code.

\begin{thebibliography}{9}

\bibitem[Bishop (2009)]{bishop}
 Bishop, Christopher M.\textit{Pattern Recognition and Machine Learning}. Springer, Cambridge, U.K., 2006.
 
\bibitem[Scholkopf (1998)]{original_paper}
B. Scholkopf, A. Smola, and K.-R. Muller.
\textit{Nonlinear component analysis as a kernel eigenvalue problem.}  Berlin, Germany., 1998.

\bibitem[Shlens (2014)]{shlens}
 Shlens, Jonathon. \textit{A Tutorial on Principal Component Analysis}.Google Research, Mountain View, USA, 2014.

\bibitem[Cardes (2009)]{cardes}
 Candes, Emmanuel J.; Li, Xiaodong ; Ma, Yi; Wright,John. \textit{Robust Principal Component Analysis?} J.ACM, 58(11):1-37, 2009 
 
\bibitem[Lu (2008)]{lu} Lu, Haiping; Plataniotis, Konstantinos N.; Venetsanopoulos,  
 Anastasios N. 
 \textit{MPCA: Multilinear Principal Component Analysis of Tensor Objects}.  IEEE Transactions on Neural Networks, 19(1):18-39, 2008
 
\bibitem[Sahambi (2003)]{sahambi} 
 Sahambi, Harkirat S.; Khorasani, Khakhayar. \textit{A neural-network appearance-based 3-D object recognition using independent component analysis} IEEE Transactions on Neural Networks, 14(1): 138-149, 2003. 
 
\bibitem[Green (2004)]{green} 
 Green, Richard D.; Guan, Ling. \textit{Quantifying and recognizing human movement patterns from monocular video images - Part II: Applications to biometrics}. IEEE Transactions on Circuits and Systems for Video Technology, 14(2): 191-198, 2004. 
 
\bibitem[Hastie (1989)]{hastie}
 Hastie, Trevor; Stuetzle, Werner. \textit{Principal Curves}. Journal of the American Statistical Association, 84(406): 502-516, 1989.

\bibitem[Gorban (2007)]{gorban}
 Gorban,  Alexander N.; Kegl, Balazs; Wunsch, Donald C.; Zinovyev, Andrei. \textit{Principal Manifolds for Data Visualisation and Dimension Reduction}. Springer, Berlin – Heidelberg – New York, 2007. 
\end{thebibliography}

\end{document}